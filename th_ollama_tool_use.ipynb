{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toolhouse with Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's set up the Jupyter environment to handle asynchronous programming and enables the display of rich HTML content.\n",
    "\n",
    "1. **`import nest_asyncio`**: Imports the `nest_asyncio` library, which allows for the smooth handling of asynchronous tasks in environments like Google Colab, where nested event loops might otherwise cause issues.\n",
    "\n",
    "2. **`nest_asyncio.apply()`**: Applies a patch that enables running asynchronous code without conflicts. This is particularly useful when working with `asyncio` in notebooks, allowing for proper execution of asynchronous tasks.\n",
    "\n",
    "3. **`from IPython.display import display, HTML`**: Imports functions from IPython’s display module that allow for the rendering of rich media content (like HTML) directly within the notebook. This enables the output of formatted HTML, which can be used for interactive content or visually enriched outputs.\n",
    "\n",
    "This setup ensures that you can effectively run and display asynchronous code alongside HTML content in the same notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure you have Ollama installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/ollama\n"
     ]
    }
   ],
   "source": [
    "!which ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install Ollama: https://ollama.com/download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the ollama package from the Python Package Index (PyPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ollama --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure you have Mistral in Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME          \tID          \tSIZE  \tMODIFIED     \n",
      "mistral:latest\tf974a74358d6\t4.1 GB\t46 hours ago\t\n",
      "llama3:latest \t365c0bd3c000\t4.7 GB\t2 months ago\t\n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install Mistral on Ollama run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling ff82381e2bea... 100% ▕████████████████▏ 4.1 GB                         \n",
      "pulling 43070e2d4e53... 100% ▕████████████████▏  11 KB                         \n",
      "pulling 491dfa501e59... 100% ▕████████████████▏  801 B                         \n",
      "pulling ed11eda7790d... 100% ▕████████████████▏   30 B                         \n",
      "pulling 42347cd80dc8... 100% ▕████████████████▏  485 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n",
      "\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest "
     ]
    }
   ],
   "source": [
    "!ollama pull mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "# Prompt the user to enter the Exa API key securely\n",
    "EXA_API_KEY = getpass(\"Enter your Exa API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the Exa AI search function using a direct web search query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Title: Articles about hot | Gothamist\n",
      "    - URL: https://gothamist.com/tags/hot\n",
      "    - Published Date: 2021-07-06T00:00:00.000Z\n",
      "    - Author: Jen Chung\n",
      "    - Text Extracted: \"NYC is under heat advisory from 11 a.m. on Tuesday until 8 p.m. on Wednesday. We're talking in the mid-90s, but with a real feel of melting in place. It will feel close to 100 degrees with humidity. It's not the REAL unofficial start of summer until the humidity hits. Temperatures are expected to hit in the mid-90s Saturday, Sunday and Monday, and the heat index is expected to be above 100.\"\n",
      "\n",
      "2. Title: Articles about foggy | Gothamist\n",
      "    - URL: https://gothamist.com/tags/foggy\n",
      "    - Published Date: 2014-01-12T00:00:00.000Z\n",
      "    - Author: Ben Yakas\n",
      "    - Text Extracted: \"Check out some gorgeous photos of the city enveloped in a thick coat of low-lying stratus clouds.\"\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import ollama\n",
    "import asyncio\n",
    "import requests\n",
    "\n",
    "# Web search using Exa AI\n",
    "def search_web(query: str) -> str:\n",
    "  headers = {\n",
    "    'accept': 'application/json',\n",
    "    'content-type': 'application/json',\n",
    "    'x-api-key': EXA_API_KEY,\n",
    "  }\n",
    "  data = {\n",
    "    \"query\": query,\n",
    "    \"type\": \"neural\",\n",
    "    \"useAutoprompt\": True,\n",
    "    \"numResults\": 10,\n",
    "    \"contents\": {\n",
    "      \"text\": True\n",
    "    }\n",
    "  }\n",
    "  # Correct Exa AI search endpoint\n",
    "  response = requests.post('https://api.exa.ai/search', headers=headers, json=data)\n",
    "  \n",
    "  # Check if the response is valid JSON\n",
    "  try:\n",
    "    response.raise_for_status()  # Raise an error for bad status codes\n",
    "    response_json = response.json()\n",
    "  except requests.exceptions.HTTPError as http_err:\n",
    "    print(f\"HTTP error occurred: {http_err}\")\n",
    "    return json.dumps({'error': 'HTTP error occurred'})\n",
    "  except json.JSONDecodeError:\n",
    "    print(\"Failed to decode JSON. Response content:\")\n",
    "    print(response.text)\n",
    "    return json.dumps({'error': 'Failed to decode JSON response'})\n",
    "\n",
    "  return json.dumps(response_json)\n",
    "\n",
    "async def run(model: str):\n",
    "  client = ollama.AsyncClient()\n",
    "  # Initialize conversation with a user query\n",
    "  messages = [{'role': 'user', 'content': 'Search the web for \"current weather in New York\"'}]\n",
    "\n",
    "  # First API call: Send the query and function description to the model\n",
    "  response = await client.chat(\n",
    "    model=model,\n",
    "    messages=messages,\n",
    "    tools=[\n",
    "      {\n",
    "        'type': 'function',\n",
    "        'function': {\n",
    "          'name': 'search_web',\n",
    "          'description': 'Search the web for a given query',\n",
    "          'parameters': {\n",
    "            'type': 'object',\n",
    "            'properties': {\n",
    "              'query': {\n",
    "                'type': 'string',\n",
    "                'description': 'The search query',\n",
    "              },\n",
    "            },\n",
    "            'required': ['query'],\n",
    "          },\n",
    "        },\n",
    "      },\n",
    "    ],\n",
    "  )\n",
    "\n",
    "  # Add the model's response to the conversation history\n",
    "  messages.append(response['message'])\n",
    "\n",
    "  # Check if the model decided to use the provided function\n",
    "  if not response['message'].get('tool_calls'):\n",
    "    print(\"The model didn't use the function. Its response was:\")\n",
    "    print(response['message']['content'])\n",
    "    return\n",
    "\n",
    "  # Process function calls made by the model\n",
    "  if response['message'].get('tool_calls'):\n",
    "    available_functions = {\n",
    "      'search_web': search_web,\n",
    "    }\n",
    "    for tool in response['message']['tool_calls']:\n",
    "      function_to_call = available_functions[tool['function']['name']]\n",
    "      function_response = function_to_call(tool['function']['arguments']['query'])\n",
    "      # Add function response to the conversation\n",
    "      messages.append(\n",
    "        {\n",
    "          'role': 'tool',\n",
    "          'content': function_response,\n",
    "        }\n",
    "      )\n",
    "\n",
    "  # Second API call: Get final response from the model\n",
    "  final_response = await client.chat(model=model, messages=messages)\n",
    "  print(final_response['message']['content'])\n",
    "\n",
    "\n",
    "# Run the async function\n",
    "asyncio.run(run('mistral'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the Toolhouse package from the Python Package Index (PyPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install toolhouse --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "# Prompt the user to enter the Exa API key securely\n",
    "TH_API_KEY = getpass(\"Enter your ToolHouse API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check available tools in ToolHouse store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available tools:\n",
      "- exa_web_search\n",
      "- current_time\n",
      "- pdf2csv\n"
     ]
    }
   ],
   "source": [
    "# List available tools\n",
    "available_tools = th.get_tools()\n",
    "print(\"Available tools:\")\n",
    "for tool in available_tools:\n",
    "    print(f\"- {tool['function']['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original response from Ollama:\n",
      "{\n",
      "  \"model\": \"mistral\",\n",
      "  \"created_at\": \"2024-09-06T13:32:34.951406Z\",\n",
      "  \"message\": {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"\",\n",
      "    \"tool_calls\": [\n",
      "      {\n",
      "        \"function\": {\n",
      "          \"name\": \"exa_web_search\",\n",
      "          \"arguments\": {\n",
      "            \"query\": \"current weather in New York\"\n",
      "          }\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"function\": {\n",
      "          \"name\": \"exa_web_search\",\n",
      "          \"arguments\": {\n",
      "            \"num_results\": 5,\n",
      "            \"query\": \"current weather in New York\"\n",
      "          }\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"function\": {\n",
      "          \"name\": \"pdf2csv\",\n",
      "          \"arguments\": {\n",
      "            \"pdf_url\": \"<YOUR_PDF_URL>\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"done_reason\": \"stop\",\n",
      "  \"done\": true,\n",
      "  \"total_duration\": 3612544417,\n",
      "  \"load_duration\": 14271834,\n",
      "  \"prompt_eval_count\": 377,\n",
      "  \"prompt_eval_duration\": 930490000,\n",
      "  \"eval_count\": 149,\n",
      "  \"eval_duration\": 2666123000\n",
      "}\n",
      "\n",
      "Tool calls found in the response:\n",
      "[\n",
      "  {\n",
      "    \"function\": {\n",
      "      \"name\": \"exa_web_search\",\n",
      "      \"arguments\": {\n",
      "        \"query\": \"current weather in New York\"\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"function\": {\n",
      "      \"name\": \"exa_web_search\",\n",
      "      \"arguments\": {\n",
      "        \"num_results\": 5,\n",
      "        \"query\": \"current weather in New York\"\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"function\": {\n",
      "      \"name\": \"pdf2csv\",\n",
      "      \"arguments\": {\n",
      "        \"pdf_url\": \"<YOUR_PDF_URL>\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "]\n",
      "\n",
      "Adapted response for Toolhouse:\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"\",\n",
      "        \"tool_calls\": [\n",
      "          {\n",
      "            \"function\": {\n",
      "              \"name\": \"exa_web_search\",\n",
      "              \"arguments\": {\n",
      "                \"query\": \"current weather in New York\"\n",
      "              }\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"function\": {\n",
      "              \"name\": \"exa_web_search\",\n",
      "              \"arguments\": {\n",
      "                \"num_results\": 5,\n",
      "                \"query\": \"current weather in New York\"\n",
      "              }\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"function\": {\n",
      "              \"name\": \"pdf2csv\",\n",
      "              \"arguments\": {\n",
      "                \"pdf_url\": \"<YOUR_PDF_URL>\"\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"finish_reason\": \"tool_calls\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Error running tools: 'dict' object has no attribute 'choices'\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "import json\n",
    "import ollama\n",
    "import asyncio\n",
    "from toolhouse import Toolhouse\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize Toolhouse\n",
    "th = Toolhouse(access_token=TH_API_KEY)\n",
    "\n",
    "# Define the Ollama model we want to use\n",
    "MODEL = 'mistral'\n",
    "\n",
    "async def run_with_toolhouse(model: str):\n",
    "    client = ollama.AsyncClient()\n",
    "    # Initialize conversation with a user query\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Search the web for current weather in New York using exa_web_search\"\n",
    "    }]\n",
    "\n",
    "    # First API call: Send the query and function description to the model\n",
    "    response = await client.chat(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        tools=th.get_tools(),  # This includes the Code Execution tool\n",
    "    )\n",
    "\n",
    "    print(\"Original response from Ollama:\")\n",
    "    print(json.dumps(response, indent=2))\n",
    "\n",
    "    # Add the model's response to the conversation history\n",
    "    messages.append(response['message'])\n",
    "\n",
    "    # Check if the model decided to use the provided function\n",
    "    if response['message'].get('tool_calls'):\n",
    "        print(\"\\nTool calls found in the response:\")\n",
    "        print(json.dumps(response['message']['tool_calls'], indent=2))\n",
    "\n",
    "        # Adapt the response to match what Toolhouse expects\n",
    "        adapted_response = {\n",
    "            'choices': [{\n",
    "                'message': response['message'],\n",
    "                'finish_reason': 'tool_calls'\n",
    "            }]\n",
    "        }\n",
    "        print(\"\\nAdapted response for Toolhouse:\")\n",
    "        print(json.dumps(adapted_response, indent=2))\n",
    "\n",
    "        # Process function calls made by the model\n",
    "        try:\n",
    "            tool_responses = th.run_tools(adapted_response)\n",
    "            print(\"\\nTool responses:\")\n",
    "            print(json.dumps(tool_responses, indent=2))\n",
    "            messages.extend(tool_responses)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError running tools: {str(e)}\")\n",
    "            return\n",
    "\n",
    "    # Second API call: Get final response from the model\n",
    "    final_response = await client.chat(model=model, messages=messages)\n",
    "    print(\"\\nFinal response:\")\n",
    "    print(final_response['message']['content'])\n",
    "\n",
    "# Run the async function\n",
    "await run_with_toolhouse(MODEL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
