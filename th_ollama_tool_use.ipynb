{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toolhouse with Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's set up the Jupyter environment to handle asynchronous programming and enables the display of rich HTML content.\n",
    "\n",
    "1. **`import nest_asyncio`**: Imports the `nest_asyncio` library, which allows for the smooth handling of asynchronous tasks in environments like Google Colab, where nested event loops might otherwise cause issues.\n",
    "\n",
    "2. **`nest_asyncio.apply()`**: Applies a patch that enables running asynchronous code without conflicts. This is particularly useful when working with `asyncio` in notebooks, allowing for proper execution of asynchronous tasks.\n",
    "\n",
    "3. **`from IPython.display import display, HTML`**: Imports functions from IPythonâ€™s display module that allow for the rendering of rich media content (like HTML) directly within the notebook. This enables the output of formatted HTML, which can be used for interactive content or visually enriched outputs.\n",
    "\n",
    "This setup ensures that you can effectively run and display asynchronous code alongside HTML content in the same notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure you have Ollama installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/ollama\n"
     ]
    }
   ],
   "source": [
    "!which ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install Ollama: https://ollama.com/download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the ollama package from the Python Package Index (PyPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ollama --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure you Ollama is running and Mistral is available in Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME          \tID          \tSIZE  \tMODIFIED     \n",
      "mistral:latest\tf974a74358d6\t4.1 GB\t40 hours ago\t\n",
      "llama3:latest \t365c0bd3c000\t4.7 GB\t2 months ago\t\n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install Mistral on Ollama run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest â ‹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ¹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ¸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ´ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling ff82381e2bea... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– 4.1 GB                         \n",
      "pulling 43070e2d4e53... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  11 KB                         \n",
      "pulling 491dfa501e59... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  801 B                         \n",
      "pulling ed11eda7790d... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   30 B                         \n",
      "pulling 42347cd80dc8... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  485 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "!ollama pull mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "# Prompt the user to enter the Exa API key securely\n",
    "EXA_API_KEY = getpass(\"Enter your Exa API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the Exa AI search function + Ollama using a direct web search query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The weather forecast for the next few days is as follows:\n",
      "\n",
      "   - Today (Sun): Sunrise at 6:45 am, sunset at 5:35 pm. The UV index is 3 of 10. It will be partly cloudy with low temperatures near 30F and winds WSW at 5 to 10 mph. Humidity will be around 50%. There will be a new moon rising at 6:45 am and setting at 4:51 pm.\n",
      "   - Monday (Mon): Sunrise at 6:43 am, sunset at 5:36 pm. The UV index is 3 of 10. It will be day intervals of clouds and sunshine with high temperatures of 46F and winds SW at 5 to 10 mph. Humidity will be around 42%. There will be a new moon rising at 7:17 am and setting at 6:12 pm.\n",
      "   - Tuesday (Mon): Sunrise at 6:43 am, sunset at 5:36 pm. The UV index is 0 of 10. It will be a few clouds with low temperatures of 34F and winds SW at 5 to 10 mph. Humidity will be around 54%. There will be a new moon rising at 7:17 am and setting at 6:12 pm.\n",
      "\n",
      "  2. Here are some articles related to the weather:\n",
      "\n",
      "   - Article 1: \"Gorgeous Photos of New York City Enveloped in Fog\" (https://gothamist.com/tags/foggy), published on January 12, 2014 by Ben Yakas. This article features some beautiful photos of New York City under a thick coat of low-lying stratus clouds.\n",
      "   - Article 2: \"NYC Under Heat Advisory as Temperatures Reach Mid-90s\" (https://gothamist.com/tags/hot), published on July 6, 2021 by Jen Chung. This article discusses the heat advisory in NYC from 11 am Tuesday until 8 pm Wednesday, with temperatures reaching mid-90s and a real feel of over 100 degrees. It also mentions that four of the ten record-holding hot Julys have occurred since 2010, indicating that the Earth's atmosphere is warming at an alarming rate.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import ollama\n",
    "import asyncio\n",
    "import requests\n",
    "\n",
    "# Web search using Exa AI\n",
    "def search_web(query: str) -> str:\n",
    "  headers = {\n",
    "    'accept': 'application/json',\n",
    "    'content-type': 'application/json',\n",
    "    'x-api-key': EXA_API_KEY,\n",
    "  }\n",
    "  data = {\n",
    "    \"query\": query,\n",
    "    \"type\": \"neural\",\n",
    "    \"useAutoprompt\": True,\n",
    "    \"numResults\": 10,\n",
    "    \"contents\": {\n",
    "      \"text\": True\n",
    "    }\n",
    "  }\n",
    "  # Correct Exa AI search endpoint\n",
    "  response = requests.post('https://api.exa.ai/search', headers=headers, json=data)\n",
    "  \n",
    "  # Check if the response is valid JSON\n",
    "  try:\n",
    "    response.raise_for_status()  # Raise an error for bad status codes\n",
    "    response_json = response.json()\n",
    "  except requests.exceptions.HTTPError as http_err:\n",
    "    print(f\"HTTP error occurred: {http_err}\")\n",
    "    return json.dumps({'error': 'HTTP error occurred'})\n",
    "  except json.JSONDecodeError:\n",
    "    print(\"Failed to decode JSON. Response content:\")\n",
    "    print(response.text)\n",
    "    return json.dumps({'error': 'Failed to decode JSON response'})\n",
    "\n",
    "  return json.dumps(response_json)\n",
    "\n",
    "async def run(model: str):\n",
    "  client = ollama.AsyncClient()\n",
    "  # Initialize conversation with a user query\n",
    "  messages = [{'role': 'user', 'content': 'Search the web for \"current weather in New York\"'}]\n",
    "\n",
    "  # First API call: Send the query and function description to the model\n",
    "  response = await client.chat(\n",
    "    model=model,\n",
    "    messages=messages,\n",
    "    tools=[\n",
    "      {\n",
    "        'type': 'function',\n",
    "        'function': {\n",
    "          'name': 'search_web',\n",
    "          'description': 'Search the web for a given query',\n",
    "          'parameters': {\n",
    "            'type': 'object',\n",
    "            'properties': {\n",
    "              'query': {\n",
    "                'type': 'string',\n",
    "                'description': 'The search query',\n",
    "              },\n",
    "            },\n",
    "            'required': ['query'],\n",
    "          },\n",
    "        },\n",
    "      },\n",
    "    ],\n",
    "  )\n",
    "\n",
    "  # Add the model's response to the conversation history\n",
    "  messages.append(response['message'])\n",
    "\n",
    "  # Check if the model decided to use the provided function\n",
    "  if not response['message'].get('tool_calls'):\n",
    "    print(\"The model didn't use the function. Its response was:\")\n",
    "    print(response['message']['content'])\n",
    "    return\n",
    "\n",
    "  # Process function calls made by the model\n",
    "  if response['message'].get('tool_calls'):\n",
    "    available_functions = {\n",
    "      'search_web': search_web,\n",
    "    }\n",
    "    for tool in response['message']['tool_calls']:\n",
    "      function_to_call = available_functions[tool['function']['name']]\n",
    "      function_response = function_to_call(tool['function']['arguments']['query'])\n",
    "      # Add function response to the conversation\n",
    "      messages.append(\n",
    "        {\n",
    "          'role': 'tool',\n",
    "          'content': function_response,\n",
    "        }\n",
    "      )\n",
    "\n",
    "  # Second API call: Get final response from the model\n",
    "  final_response = await client.chat(model=model, messages=messages)\n",
    "  print(final_response['message']['content'])\n",
    "\n",
    "\n",
    "# Run the async function\n",
    "asyncio.run(run('mistral'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the Toolhouse package from the Python Package Index (PyPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install toolhouse --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "# Prompt the user to enter the Exa API key securely\n",
    "TH_API_KEY = getpass(\"Enter your ToolHouse API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check available tools in your personal ToolHouse \"Tool Store\" https://app.toolhouse.ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available tools:\n",
      "- exa_web_search\n",
      "- scraper\n"
     ]
    }
   ],
   "source": [
    "from toolhouse import Toolhouse\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize Toolhouse\n",
    "th = Toolhouse(access_token=TH_API_KEY)\n",
    "\n",
    "# List available tools\n",
    "available_tools = th.get_tools()\n",
    "print(\"Available tools:\")\n",
    "for tool in available_tools:\n",
    "    print(f\"- {tool['function']['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the toolhouse call with GPT-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "# Prompt the user to enter the OpenAI API key securely\n",
    "openai_api_key = getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are some current weather details and forecasts for New York:\n",
      "\n",
      "1. **National Weather Service** \n",
      "   - **Current Condition**: Mostly cloudy with highs in the mid-30s.\n",
      "   - **Forecast**: Rain showers are likely on Tuesday and Wednesday, with a chance of snow and rain showers on Friday and Saturday. Temperatures will be in the upper 40s on Tuesday and Thursday.\n",
      "   - [View on National Weather Service](https://forecast.weather.gov/MapClick.php?zoneid=NYZ010).\n",
      "\n",
      "2. **DoINeedAJacket.com**\n",
      "   - **Current Temperature**: 39Â°F (4Â°C).\n",
      "   - **Forecast**: Expect mostly cloudy weather, with a low of 34Â°F (1Â°C) and a high of 45Â°F (7Â°C) tomorrow. Thereâ€™s a 16% chance of rain.\n",
      "   - [Check the jacket recommendation](https://doineedajacket.com/weather/10016).\n",
      "\n",
      "3. **BBC Weather**\n",
      "   - **Current Condition**: Sunny intervals with a gentle breeze.\n",
      "   - **Forecast**: A detailed 10-day forecast including information on wind speed, direction, and mixture of sunny intervals, light cloud, and light rain.\n",
      "   - [See the BBC Weather forecast](https://www.bbc.com/weather/5128581).\n",
      "\n",
      "If you need more specific information, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List\n",
    "# ðŸ‘‹ Make sure you've also installed the OpenAI SDK through: pip install openai\n",
    "from openai import OpenAI\n",
    "from toolhouse import Toolhouse\n",
    "\n",
    "# Let's set our API Keys.\n",
    "# Please remember to use a safer system to store your API KEYS \n",
    "# after finishing the quick start.\n",
    "client = OpenAI(api_key='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')\n",
    "th = Toolhouse(access_token='3e76c9b1-427e-4df6-ace8-7b86a03e776b',\n",
    "provider=\"openai\")\n",
    "\n",
    "# Define the OpenAI model we want to use\n",
    "MODEL = 'gpt-4o-mini'\n",
    "\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\":\n",
    "        \"Search the web for current weather in New York using exa_web_search\"\n",
    "}]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=MODEL,\n",
    "  messages=messages,\n",
    "  # Passes Code Execution as a tool\n",
    "  tools=th.get_tools()\n",
    ")\n",
    "\n",
    "# Runs the Code Execution tool, gets the result, \n",
    "# and appends it to the context\n",
    "messages += th.run_tools(response)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=MODEL,\n",
    "  messages=messages,\n",
    "  tools=th.get_tools()\n",
    ")\n",
    "# Prints the response with the answer\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the toolhouse call with Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'choices'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28mprint\u001b[39m(final_response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Run the async function\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m run_with_toolhouse(MODEL)\n",
      "Cell \u001b[0;32mIn[4], line 44\u001b[0m, in \u001b[0;36mrun_with_toolhouse\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     36\u001b[0m     adapted_response \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m: [{\n\u001b[1;32m     38\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m: response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinish_reason\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtool_calls\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     40\u001b[0m         }]\n\u001b[1;32m     41\u001b[0m     }\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# Process function calls made by the model\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     tool_responses \u001b[38;5;241m=\u001b[39m \u001b[43mth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43madapted_response\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     messages\u001b[38;5;241m.\u001b[39mextend(tool_responses)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Second API call: Get final response from the model\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.12/site-packages/toolhouse/sdk.py:153\u001b[0m, in \u001b[0;36mToolhouse.run_tools\u001b[0;34m(self, response, append)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[0;32m--> 153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoices\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mfinish_reason \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtool_calls\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[1;32m    155\u001b[0m response_message \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'choices'"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "import json\n",
    "import ollama\n",
    "import asyncio\n",
    "from toolhouse import Toolhouse\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize Toolhouse\n",
    "th = Toolhouse(access_token='3e76c9b1-427e-4df6-ace8-7b86a03e776b')\n",
    "\n",
    "# Define the Ollama model we want to use\n",
    "MODEL = 'mistral'\n",
    "\n",
    "async def run_with_toolhouse(model: str):\n",
    "    client = ollama.AsyncClient()\n",
    "    # Initialize conversation with a user query\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Search the web for current weather in New York using exa_web_search\"\n",
    "    }]\n",
    "\n",
    "    # First API call: Send the query and function description to the model\n",
    "    response = await client.chat(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        tools=th.get_tools(),  # This includes the remote tools from Toolhouse\n",
    "    )\n",
    "\n",
    "    # Add the model's response to the conversation history\n",
    "    messages.append(response['message'])\n",
    "\n",
    "    # Check if the model decided to use the provided function\n",
    "    if response['message'].get('tool_calls'):\n",
    "        # Adapt the response to match what Toolhouse expects\n",
    "        adapted_response = {\n",
    "            'choices': [{\n",
    "                'message': response['message'],\n",
    "                'finish_reason': 'tool_calls'\n",
    "            }]\n",
    "        }\n",
    "\n",
    "        # Process function calls made by the model\n",
    "        tool_responses = th.run_tools(adapted_response)\n",
    "        messages.extend(tool_responses)\n",
    "\n",
    "    # Second API call: Get final response from the model\n",
    "    final_response = await client.chat(model=model, messages=messages)\n",
    "    print(final_response['message']['content'])\n",
    "\n",
    "# Run the async function\n",
    "await run_with_toolhouse(MODEL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
