{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toolhouse with Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up the Jupyter environment to handle asynchronous programming and enables the display of rich HTML content.\n",
    "\n",
    "1. **`import nest_asyncio`**: Imports the `nest_asyncio` library, which allows for the smooth handling of asynchronous tasks in environments like Google Colab, where nested event loops might otherwise cause issues.\n",
    "\n",
    "2. **`nest_asyncio.apply()`**: Applies a patch that enables running asynchronous code without conflicts. This is particularly useful when working with `asyncio` in notebooks, allowing for proper execution of asynchronous tasks.\n",
    "\n",
    "3. **`from IPython.display import display, HTML`**: Imports functions from IPython’s display module that allow for the rendering of rich media content (like HTML) directly within the notebook. This enables the output of formatted HTML, which can be used for interactive content or visually enriched outputs.\n",
    "\n",
    "This setup ensures that you can effectively run and display asynchronous code alongside HTML content in the same notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure you have Ollama installed locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/ollama\n"
     ]
    }
   ],
   "source": [
    "!which ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install Ollama: https://ollama.com/download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the ollama package from the Python Package Index (PyPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ollama --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure you Ollama is running and Llama 3.1 is available in Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME           \tID          \tSIZE  \tMODIFIED    \n",
      "llama3.1:latest\tf66fc8dc39ea\t4.7 GB\t6 hours ago\t\n",
      "mistral:latest \tf974a74358d6\t4.1 GB\t7 hours ago\t\n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install Mistral on Ollama run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 8eeb52dfb3bb... 100% ▕████████████████▏ 4.7 GB                         \n",
      "pulling 73b313b5552d... 100% ▕████████████████▏ 1.4 KB                         \n",
      "pulling 0ba8f0e314b4... 100% ▕████████████████▏  12 KB                         \n",
      "pulling 56bb8bd477a5... 100% ▕████████████████▏   96 B                         \n",
      "pulling 1a4c3c319823... 100% ▕████████████████▏  485 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "!ollama pull llama3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the Toolhouse package from the Python Package Index (PyPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install toolhouse --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check available tools in your personal ToolHouse \"Tool Store\" https://app.toolhouse.ai/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "insert ToolHouse API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "# Prompt the user to enter the Exa API key securely\n",
    "TH_API_KEY = getpass(\"Enter your ToolHouse API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available tools:\n",
      "- exa_web_search\n",
      "- scraper\n"
     ]
    }
   ],
   "source": [
    "from toolhouse import Toolhouse\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize Toolhouse\n",
    "th = Toolhouse(access_token=TH_API_KEY)\n",
    "\n",
    "# List available tools\n",
    "available_tools = th.get_tools()\n",
    "print(\"Available tools:\")\n",
    "for tool in available_tools:\n",
    "    print(f\"- {tool['function']['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the toolhouse call with Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toolhouse SDK initialized successfully.\n",
      "\n",
      "Available tools in Toolhouse store:\n",
      "- exa_web_search\n",
      "- scraper\n",
      "\n",
      "Ollama client initialized via OpenAI-compatible API.\n",
      "Using model: llama3.1\n",
      "Initial message: Search the web for current weather in New York using exa_web_search\n",
      "Sending initial completion request...\n",
      "Initial completion request successful.\n",
      "Running tools...\n",
      "Tool execution successful. Added 2 new message(s) to the conversation.\n",
      "Sending final completion request...\n",
      "Final completion request successful.\n",
      "\n",
      "Final response:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Current Weather in New York:\n",
       "Based on the results from exa_web_search, here is an update of current weather conditions and forecast :\n",
       "\n",
       "**Warmest part of September to hit NYC this week:** Gothamist reported unusually warm temperatures are expected in NYC for this time of the year.\n",
       "\n",
       " Forecast: **Partially Cloudy Skies, High Temperature in Mid-30s**\n",
       "\n",
       "The most recent National Weather Service (NWS) update said that NYC will experience mostly cloudy skies today. According to their forecasts:\n",
       "High temperature was at mid-30 degrees.\n",
       "\n",
       "\n",
       "**Cold snaps expected over weekend**: The NWS also shared an updated weather forecast.\n",
       "\n",
       "* Expect low of 34°F on Monday, with highs around 45°F.\n",
       " There is a chance of rain showers tomorrow night and into Tuesday evening.\n",
       "\n",
       "\n",
       "\n",
       "This NYC weather page states it is **chilly throughout the day. It has been partly cloudy today**, which was found within the web search results when using exa_web_search and its response tool to look at how much data will be provided upon tool execution.\n",
       "\n",
       "Weather.com had an article about the 12 separate seasons in the city New York , with each containing different weather conditions.</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install toolhouse openai --quiet\n",
    "\n",
    "import nest_asyncio\n",
    "from openai import OpenAI\n",
    "from toolhouse import Toolhouse\n",
    "import sys\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize the Toolhouse SDK\n",
    "try:\n",
    "    th = Toolhouse(access_token='3e76c9b1-427e-4df6-ace8-7b86a03e776b', provider=\"openai\")\n",
    "    print(\"Toolhouse SDK initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing Toolhouse SDK: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Print available tools\n",
    "print(\"\\nAvailable tools in Toolhouse store:\")\n",
    "available_tools = th.get_tools()\n",
    "for tool in available_tools:\n",
    "    print(f\"- {tool['function']['name']}\")\n",
    "print()\n",
    "\n",
    "# Initialize the OpenAI client with Ollama's endpoint\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"ollama\"  # Required but unused\n",
    ")\n",
    "print(\"Ollama client initialized via OpenAI-compatible API.\")\n",
    "\n",
    "# Define the model to use\n",
    "MODEL = \"llama3.1\"  # Changed to llama3.1\n",
    "print(f\"Using model: {MODEL}\")\n",
    "\n",
    "# Define the messages to send to the model\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Search the web for current weather in New York using exa_web_search\"\n",
    "    }\n",
    "]\n",
    "print(f\"Initial message: {messages[0]['content']}\")\n",
    "\n",
    "# Create the completion request\n",
    "try:\n",
    "    print(\"Sending initial completion request...\")\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=messages,\n",
    "        tools=available_tools,\n",
    "    )\n",
    "    print(\"Initial completion request successful.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in initial completion request: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Run the remote tool and get the result\n",
    "try:\n",
    "    print(\"Running tools...\")\n",
    "    tool_run = th.run_tools(response)\n",
    "    messages += tool_run\n",
    "    print(f\"Tool execution successful. Added {len(tool_run)} new message(s) to the conversation.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error running tools: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Create the final completion request with the updated messages\n",
    "try:\n",
    "    print(\"Sending final completion request...\")\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=messages,\n",
    "        tools=available_tools,\n",
    "    )\n",
    "    print(\"Final completion request successful.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in final completion request: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Print the response with the answer\n",
    "if response.choices and response.choices[0].message.content:\n",
    "    print(\"\\nFinal response:\")\n",
    "    display(HTML(f\"<pre>{response.choices[0].message.content}</pre>\"))\n",
    "else:\n",
    "    print(\"No response content found in the model's reply.\")\n",
    "    print(\"Response object:\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
