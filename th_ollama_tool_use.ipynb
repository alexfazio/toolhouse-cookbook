{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toolhouse with Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's set up the Jupyter environment to handle asynchronous programming and enables the display of rich HTML content.\n",
    "\n",
    "1. **`import nest_asyncio`**: Imports the `nest_asyncio` library, which allows for the smooth handling of asynchronous tasks in environments like Google Colab, where nested event loops might otherwise cause issues.\n",
    "\n",
    "2. **`nest_asyncio.apply()`**: Applies a patch that enables running asynchronous code without conflicts. This is particularly useful when working with `asyncio` in notebooks, allowing for proper execution of asynchronous tasks.\n",
    "\n",
    "3. **`from IPython.display import display, HTML`**: Imports functions from IPython’s display module that allow for the rendering of rich media content (like HTML) directly within the notebook. This enables the output of formatted HTML, which can be used for interactive content or visually enriched outputs.\n",
    "\n",
    "This setup ensures that you can effectively run and display asynchronous code alongside HTML content in the same notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure you have Ollama installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/ollama\n"
     ]
    }
   ],
   "source": [
    "!which ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install Ollama: https://ollama.com/download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the ollama package from the Python Package Index (PyPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ollama --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure you Ollama is running and Mistral is available in Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME          \tID          \tSIZE  \tMODIFIED     \n",
      "mistral:latest\tf974a74358d6\t4.1 GB\t21 hours ago\t\n",
      "llama3:latest \t365c0bd3c000\t4.7 GB\t2 months ago\t\n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install Mistral on Ollama run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling ff82381e2bea... 100% ▕████████████████▏ 4.1 GB                         \n",
      "pulling 43070e2d4e53... 100% ▕████████████████▏  11 KB                         \n",
      "pulling 491dfa501e59... 100% ▕████████████████▏  801 B                         \n",
      "pulling ed11eda7790d... 100% ▕████████████████▏   30 B                         \n",
      "pulling 42347cd80dc8... 100% ▕████████████████▏  485 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "!ollama pull mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "# Prompt the user to enter the Exa API key securely\n",
    "EXA_API_KEY = getpass(\"Enter your Exa API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the Exa AI search function using a direct web search query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The weather forecast for the next two days is as follows:\n",
      "   - Day 19 (Current day): Sunrise at 6:45 am, sunset at 5:35 pm. The UV index is 3 out of 10 and the skies are partly cloudy with a low temperature near 30F and winds WSW at 5 to 10 mph. The humidity level is 50%.\n",
      "   - Day 20 (Next day): Sunrise at 6:43 am, sunset at 5:36 pm. The UV index is 3 out of 10 and the skies are a mix of clouds and sunshine with a high temperature of 46F and winds SW at 5 to 10 mph. The humidity level is 42%.\n",
      "\n",
      "  2. Here are some articles related to your search:\n",
      "   - Title: Articles about foggy | Gothamist\n",
      "     URL: https://gothamist.com/tags/foggy\n",
      "     Published Date: January 12, 2014\n",
      "     Author: Ben Yakas\n",
      "     Text Snippet: This article features beautiful photos of New York City enveloped in fog and includes a subscription form for essential NYC news, arts, events, and food delivered to your inbox daily.\n",
      "\n",
      "   - Title: Articles about hot | Gothamist\n",
      "     URL: https://gothamist.com/tags/hot\n",
      "     Published Date: July 6, 2021\n",
      "     Author: Jen Chung\n",
      "     Text Snippet: This article discusses the upcoming heat advisory in NYC and the expected temperatures to reach mid-90s with a real feel of over 100 degrees. It also mentions the impact of global warming on heatwaves and includes a subscription form for essential NYC news.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import ollama\n",
    "import asyncio\n",
    "import requests\n",
    "\n",
    "# Web search using Exa AI\n",
    "def search_web(query: str) -> str:\n",
    "  headers = {\n",
    "    'accept': 'application/json',\n",
    "    'content-type': 'application/json',\n",
    "    'x-api-key': EXA_API_KEY,\n",
    "  }\n",
    "  data = {\n",
    "    \"query\": query,\n",
    "    \"type\": \"neural\",\n",
    "    \"useAutoprompt\": True,\n",
    "    \"numResults\": 10,\n",
    "    \"contents\": {\n",
    "      \"text\": True\n",
    "    }\n",
    "  }\n",
    "  # Correct Exa AI search endpoint\n",
    "  response = requests.post('https://api.exa.ai/search', headers=headers, json=data)\n",
    "  \n",
    "  # Check if the response is valid JSON\n",
    "  try:\n",
    "    response.raise_for_status()  # Raise an error for bad status codes\n",
    "    response_json = response.json()\n",
    "  except requests.exceptions.HTTPError as http_err:\n",
    "    print(f\"HTTP error occurred: {http_err}\")\n",
    "    return json.dumps({'error': 'HTTP error occurred'})\n",
    "  except json.JSONDecodeError:\n",
    "    print(\"Failed to decode JSON. Response content:\")\n",
    "    print(response.text)\n",
    "    return json.dumps({'error': 'Failed to decode JSON response'})\n",
    "\n",
    "  return json.dumps(response_json)\n",
    "\n",
    "async def run(model: str):\n",
    "  client = ollama.AsyncClient()\n",
    "  # Initialize conversation with a user query\n",
    "  messages = [{'role': 'user', 'content': 'Search the web for \"current weather in New York\"'}]\n",
    "\n",
    "  # First API call: Send the query and function description to the model\n",
    "  response = await client.chat(\n",
    "    model=model,\n",
    "    messages=messages,\n",
    "    tools=[\n",
    "      {\n",
    "        'type': 'function',\n",
    "        'function': {\n",
    "          'name': 'search_web',\n",
    "          'description': 'Search the web for a given query',\n",
    "          'parameters': {\n",
    "            'type': 'object',\n",
    "            'properties': {\n",
    "              'query': {\n",
    "                'type': 'string',\n",
    "                'description': 'The search query',\n",
    "              },\n",
    "            },\n",
    "            'required': ['query'],\n",
    "          },\n",
    "        },\n",
    "      },\n",
    "    ],\n",
    "  )\n",
    "\n",
    "  # Add the model's response to the conversation history\n",
    "  messages.append(response['message'])\n",
    "\n",
    "  # Check if the model decided to use the provided function\n",
    "  if not response['message'].get('tool_calls'):\n",
    "    print(\"The model didn't use the function. Its response was:\")\n",
    "    print(response['message']['content'])\n",
    "    return\n",
    "\n",
    "  # Process function calls made by the model\n",
    "  if response['message'].get('tool_calls'):\n",
    "    available_functions = {\n",
    "      'search_web': search_web,\n",
    "    }\n",
    "    for tool in response['message']['tool_calls']:\n",
    "      function_to_call = available_functions[tool['function']['name']]\n",
    "      function_response = function_to_call(tool['function']['arguments']['query'])\n",
    "      # Add function response to the conversation\n",
    "      messages.append(\n",
    "        {\n",
    "          'role': 'tool',\n",
    "          'content': function_response,\n",
    "        }\n",
    "      )\n",
    "\n",
    "  # Second API call: Get final response from the model\n",
    "  final_response = await client.chat(model=model, messages=messages)\n",
    "  print(final_response['message']['content'])\n",
    "\n",
    "\n",
    "# Run the async function\n",
    "asyncio.run(run('mistral'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the Toolhouse package from the Python Package Index (PyPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install toolhouse --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "# Prompt the user to enter the Exa API key securely\n",
    "TH_API_KEY = getpass(\"Enter your ToolHouse API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check available tools in your personal ToolHouse \"Tool Store\" https://app.toolhouse.ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available tools:\n",
      "- exa_web_search\n",
      "- scraper\n"
     ]
    }
   ],
   "source": [
    "from toolhouse import Toolhouse\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize Toolhouse\n",
    "th = Toolhouse(access_token=TH_API_KEY)\n",
    "\n",
    "# List available tools\n",
    "available_tools = th.get_tools()\n",
    "print(\"Available tools:\")\n",
    "for tool in available_tools:\n",
    "    print(f\"- {tool['function']['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the toolhouse call with GPT-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: toolhouse in /opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.12/site-packages (1.2.0)\n",
      "Requirement already satisfied: openai in /opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.12/site-packages (1.35.10)\n",
      "Requirement already satisfied: requests in /opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.12/site-packages (from toolhouse) (2.32.3)\n",
      "Requirement already satisfied: http-exceptions in /opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.12/site-packages (from toolhouse) (0.2.10)\n",
      "Requirement already satisfied: anthropic in /opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.12/site-packages (from toolhouse) (0.30.1)\n",
      "Requirement already satisfied: groq in /opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.12/site-packages (from toolhouse) (0.11.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.12/site-packages (from openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.12/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.12/site-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in /opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.12/site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.12/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.12/site-packages (from anthropic->toolhouse) (0.5.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.0 in /opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.12/site-packages (from anthropic->toolhouse) (0.19.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.12/site-packages (from requests->toolhouse) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.12/site-packages (from requests->toolhouse) (2.2.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.12/site-packages (from tokenizers>=0.13.0->anthropic->toolhouse) (0.23.4)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic->toolhouse) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic->toolhouse) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic->toolhouse) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic->toolhouse) (6.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install toolhouse openai --quiet\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from toolhouse import Toolhouse\n",
    "from getpass import getpass\n",
    "\n",
    "# Set up API keys (you'll be prompted to enter these securely)\n",
    "openai_api_key = getpass(\"Enter your OpenAI API key: \")\n",
    "toolhouse_api_key = getpass(\"Enter your Toolhouse API key: \")\n",
    "\n",
    "# Initialize OpenAI and Toolhouse clients\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "th = Toolhouse(access_token=toolhouse_api_key)\n",
    "\n",
    "# Define the OpenAI model we want to use\n",
    "MODEL = 'gpt-4'\n",
    "\n",
    "# Set up the conversation\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Generate a Python function to calculate the factorial of a number, then use it to compute the factorial of 5.\"\n",
    "}]\n",
    "\n",
    "# First API call: Send the query and function description to the model\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=messages,\n",
    "    tools=th.get_tools()  # This includes the Code Interpreter tool\n",
    ")\n",
    "\n",
    "# Add the model's response to the conversation history\n",
    "messages.append(response.choices[0].message)\n",
    "\n",
    "# Check if the model decided to use the provided tool\n",
    "if response.choices[0].message.tool_calls:\n",
    "    # Process function calls made by the model\n",
    "    tool_responses = th.run_tools(response)\n",
    "    messages.extend(tool_responses)\n",
    "\n",
    "    # Second API call: Get final response from the model\n",
    "    final_response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=messages,\n",
    "        tools=th.get_tools()\n",
    "    )\n",
    "    \n",
    "    print(\"Final response:\")\n",
    "    print(final_response.choices[0].message.content)\n",
    "else:\n",
    "    print(\"The model didn't use the tool. Its response was:\")\n",
    "    print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the toolhouse call with Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnprocessableEntityException",
     "evalue": "UnprocessableEntityException(status_code=422, message='Invalid data')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnprocessableEntityException\u001b[0m              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 66\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28mprint\u001b[39m(final_response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Run the async function\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m run_with_toolhouse(MODEL)\n",
      "Cell \u001b[0;32mIn[16], line 26\u001b[0m, in \u001b[0;36mrun_with_toolhouse\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     17\u001b[0m messages \u001b[38;5;241m=\u001b[39m [{\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSearch the web for current weather in New York using exa_web_search\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m }]\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# First API call: Send the query and function description to the model\u001b[39;00m\n\u001b[1;32m     23\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m client\u001b[38;5;241m.\u001b[39mchat(\n\u001b[1;32m     24\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     25\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[0;32m---> 26\u001b[0m     tools\u001b[38;5;241m=\u001b[39m\u001b[43mth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,  \u001b[38;5;66;03m# This includes the Code Execution tool\u001b[39;00m\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal response from Ollama:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(json\u001b[38;5;241m.\u001b[39mdumps(response, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.12/site-packages/toolhouse/sdk.py:129\u001b[0m, in \u001b[0;36mToolhouse.get_tools\u001b[0;34m(self, bundle)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;124;03mGet Tools\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbundle \u001b[38;5;241m=\u001b[39m bundle\n\u001b[0;32m--> 129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGetToolsRequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprovider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprovider\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbundle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbundle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.12/site-packages/toolhouse/services/tools.py:29\u001b[0m, in \u001b[0;36mTools.get_tools\u001b[0;34m(self, request_input)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_required_headers(headers)\n\u001b[1;32m     28\u001b[0m final_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_url_prefix \u001b[38;5;241m+\u001b[39m url_endpoint\n\u001b[0;32m---> 29\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_http\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m GetToolsResponseGuardModel\u001b[38;5;241m.\u001b[39mreturn_one_of(res)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.12/site-packages/toolhouse/net/http_client.py:175\u001b[0m, in \u001b[0;36mHTTPClient.post\u001b[0;34m(self, endpoint_url, headers, body_input, retry)\u001b[0m\n\u001b[1;32m    170\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_http_request(\n\u001b[1;32m    171\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, endpoint_url, headers, body_input\n\u001b[1;32m    172\u001b[0m         )\n\u001b[1;32m    173\u001b[0m         try_cnt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.12/site-packages/toolhouse/net/http_client.py:238\u001b[0m, in \u001b[0;36mHTTPClient._handle_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 238\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_from_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/myenv/lib/python3.12/site-packages/toolhouse/net/http_client.py:275\u001b[0m, in \u001b[0;36mHTTPClient._raise_from_status\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m server_exceptions\u001b[38;5;241m.\u001b[39mServiceUnavailableException(\n\u001b[1;32m    272\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_exception_message(response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetry-After\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    273\u001b[0m     )\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 275\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPException\u001b[38;5;241m.\u001b[39mfrom_status_code(status_code\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code)(\n\u001b[1;32m    276\u001b[0m         message\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    277\u001b[0m     )\n",
      "\u001b[0;31mUnprocessableEntityException\u001b[0m: UnprocessableEntityException(status_code=422, message='Invalid data')"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "import json\n",
    "import ollama\n",
    "import asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# temp fix\n",
    "# th.set_base_url(\"https://t2ojtd5voi.execute-api.us-west-2.amazonaws.com/v1\")\n",
    "\n",
    "# Define the Ollama model we want to use\n",
    "MODEL = 'mistral'\n",
    "\n",
    "async def run_with_toolhouse(model: str):\n",
    "    client = ollama.AsyncClient()\n",
    "    # Initialize conversation with a user query\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Search the web for current weather in New York using exa_web_search\"\n",
    "    }]\n",
    "\n",
    "    # First API call: Send the query and function description to the model\n",
    "    response = await client.chat(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        tools=th.get_tools(),  # This includes the Code Execution tool\n",
    "    )\n",
    "\n",
    "    print(\"Original response from Ollama:\")\n",
    "    print(json.dumps(response, indent=2))\n",
    "\n",
    "    # Add the model's response to the conversation history\n",
    "    messages.append(response['message'])\n",
    "\n",
    "    # Check if the model decided to use the provided function\n",
    "    if response['message'].get('tool_calls'):\n",
    "        print(\"\\nTool calls found in the response:\")\n",
    "        print(json.dumps(response['message']['tool_calls'], indent=2))\n",
    "\n",
    "        # Adapt the response to match what Toolhouse expects\n",
    "        adapted_response = {\n",
    "            'choices': [{\n",
    "                'message': response['message'],\n",
    "                'finish_reason': 'tool_calls'\n",
    "            }]\n",
    "        }\n",
    "        print(\"\\nAdapted response for Toolhouse:\")\n",
    "        print(json.dumps(adapted_response, indent=2))\n",
    "\n",
    "        # Process function calls made by the model\n",
    "        try:\n",
    "            tool_responses = th.run_tools(adapted_response)\n",
    "            print(\"\\nTool responses:\")\n",
    "            print(json.dumps(tool_responses, indent=2))\n",
    "            messages.extend(tool_responses)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError running tools: {str(e)}\")\n",
    "            return\n",
    "\n",
    "    # Second API call: Get final response from the model\n",
    "    final_response = await client.chat(model=model, messages=messages)\n",
    "    print(\"\\nFinal response:\")\n",
    "    print(final_response['message']['content'])\n",
    "\n",
    "# Run the async function\n",
    "await run_with_toolhouse(MODEL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
